<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <title>Cars Live Transcription</title>
    <style>
        body {
            font-family: Arial;
            padding: 20px;
        }

        #transcript {
            border: 1px solid #ccc;
            padding: 10px;
            height: 300px;
            overflow-y: scroll;
        }
    </style>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <h1>Audio Transcription</h1>
    <button id="startBtn">Start</button>
    <div id="transcript"></div>

    <script>
        const startBtn = document.getElementById("startBtn");
        const transcriptDiv = document.getElementById("transcript");
        let pc, dc, audioContext, workletNode;

        startBtn.onclick = async () => {
            console.log("[LOG] Starting microphone capture...");
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

            // Create RTCPeerConnection and DataChannel
            pc = new RTCPeerConnection();
            console.log("[LOG] RTCPeerConnection created.");

            dc = pc.createDataChannel("audio");
            // Assistant text ← backend
            const recognizedtextChannel = pc.createDataChannel("recognized-text-out");
            const responsetextChannel = pc.createDataChannel("response-text-out");

            const textChannel = pc.createDataChannel("text");


            // Synthesized audio ← backend
            const audioChannel = pc.createDataChannel("audio-out");
            dc.binaryType = "arraybuffer";
            dc.onopen = () => console.log("[LOG] Data channel open");
            dc.onclose = () => console.log("[LOG] Data channel closed");
            dc.onerror = (err) => console.error("[ERROR] Data channel error:", err);

            textChannel.onmessage = (event) => {
                try {
                    const msg = JSON.parse(event.data); // Unpack the JSON string

                    console.log("[TextChannel] Received message:", msg);

                    // Check if this is a context request
                    if (msg.type === "context" && msg.action === "request") {
                        console.log("[TextChannel] Backend requested context");

                        // Example payload (your app’s current context)
                        const currentContext = {
                            current_page: "dashboard",
                            page_context: "user-overview",
                            page_display: "main",
                            max_steps: 4,
                            current_step: 1,
                        };

                        // Create a response message
                        const response = {
                            type: "context",
                            action: "update",
                            payload: currentContext,
                        };

                        // Send it back to the backend
                        textChannel.send(JSON.stringify(response));
                        console.log("[TextChannel] Sent context update:", response);
                    }

                } catch (err) {
                    console.error("[TextChannel] Invalid JSON message:", err, event.data);
                }}

            // Recognized user speech
            recognizedtextChannel.onmessage = (event) => {
                const p = document.createElement("p");
                p.textContent = event.data;
                p.className = "recognized"; // optional CSS class
                transcriptDiv.appendChild(p);

                transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
            };
            let currentResponseP = null;

            recognizedtextChannel.onmessage = (event) => {
                // Create a new <p> for recognized user speech
                const p = document.createElement("p");
                p.className = "recognized";
                p.textContent = event.data;
                transcriptDiv.appendChild(p);

                // Reset response <p> for assistant
                currentResponseP = document.createElement("p");
                currentResponseP.className = "response";
                transcriptDiv.appendChild(currentResponseP);

                // Auto-scroll
                transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
            };

            responsetextChannel.onmessage = (event) => {
                const chunk = event.data;

                if (!currentResponseP) {
                    // Safety: create response <p> if missing
                    currentResponseP = document.createElement("p");
                    currentResponseP.className = "response";
                    transcriptDiv.appendChild(currentResponseP);
                }

                // Append the new chunk to the current response <p>
                currentResponseP.textContent += chunk;

                // Auto-scroll
                transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
            };


            // Create AudioContext and add worklet
            audioContext = new AudioContext();
            await audioContext.audioWorklet.addModule("processor.js");
            const source = audioContext.createMediaStreamSource(stream);

            workletNode = new AudioWorkletNode(audioContext, "audio-processor");
            source.connect(workletNode);
            workletNode.connect(audioContext.destination);

            console.log("[LOG] AudioContext and AudioWorkletNode initialized.");
            console.log("Sample rate:", audioContext.sampleRate);

            // Receive audio buffers from the worklet
            workletNode.port.onmessage = (event) => {
                const buffer = event.data;
                if (dc.readyState === "open") {
                    dc.send(buffer);
                    // console.log(`[LOG] Sent float32 chunk: ${buffer.byteLength} bytes`);
                }
            };

            // WebRTC signaling
            const offer = await pc.createOffer();
            await pc.setLocalDescription(offer);
            console.log("[LOG] Local SDP offer set.");

            const res = await fetch("http://localhost:8000/voice/offer", {
                method: "POST",
                body: JSON.stringify({ sdp: offer.sdp, type: offer.type }),
                headers: { "Content-Type": "application/json" }
            });

            const answer = await res.json();
            await pc.setRemoteDescription(answer);
            console.log("[LOG] Remote SDP answer set. Streaming should now be active.");

            transcriptDiv.innerHTML += "<p>Streaming started...</p>";
        };
    </script>
</body>

</html>